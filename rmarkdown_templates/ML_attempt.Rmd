---
title: "ML attempt"
author: "asa"
date: '2021-02-12'
output: html_document
#knit: (function(inputFile, encoding) {
#  rmarkdown::render(inputFile, encoding = encoding,output_file = "ML.html", output_dir = "../paper") })
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
# code for checking predictors
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(summarytools)) install.packages("summarytools")
if(!require(sjlabelled)) install.packages("sjlabelled")
if(!require(kableExtra)) install.packages("kableExtra")
sapply(c("tidyverse","summarytools","sjlabelled", "kableExtra"), library, character.only = TRUE)
#library(tidyverse)
#library(summarytools)
#library(sjlabelled)
#library(kableExtra)

```

```{r interest_url, echo=FALSE, include=FALSE}
## https://www.r-bloggers.com/2019/06/working-with-spss-labels-in-r/
## Reference url for methods: https://www.statmethods.net/advstats/cart.html
## Reference for the code: http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials/
## SHOULD BE READ: https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf
```

```{r data_label, echo=FALSE, include=FALSE}

# Step 1. Load data saved (following the "generate_clean_data.R)
lgbti_dta <- readRDS("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/clean/lgbti_dta.rds")
lgbti_sav <- readRDS("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/clean/lgbti_sav.rds")

# Step 2. Extract question labels from the .dta dataset and put them to the sav dataset
# a. Derive variable labels from the .dta dataset
var_labs <- map_chr(lgbti_dta, ~ attributes(.)$label)
# b. Attach labels to the .sav dataset
lgbti_sav <- set_label(lgbti_sav,var_labs)
```

```{r data_code, echo=FALSE, include=FALSE}
library(haven)
## Alternatively import data in coded format (haven::read_sav)
data_coded <- read_sav("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/raw/ZA7604_v1-0-0.sav")

## Step 1. Quick look at the data format
data_coded %>% 
  .[,1:20] %>%  # First 20 columns
  glimpse() ## <dbl+lbl></dbl+lbl> signifies that these variables have labels associated to the numeric values they hold

## Step 2. Extract HTML document fully describing the data, their categories and answer options
library(sjPlot) #sjPlot is required
var_desc<-data_coded %>% sjPlot::view_df() 
print(var_desc)
```

```{r train_test_datasets, echo=FALSE, include=FALSE}
## 1st Function: First approach to create a training and test dataset
######################  
## NOTE: There are alternative ways of producing the train dataset, e.g. giving probabilities. Currently we keep it simple
######################  

train_test_sets<-function(data, seed, perc_sample){
## Data: input data to be subsetted to create the training and test datasets
## seed: Specify initial value for the seed
## perc_sample: the percentage of the sample size to be taken for specifying the size of the train dataset
set.seed(seed)
data_excl<- data %>% filter(!is.na(open_at_work)) ## Does this work for the labeled data?
sample <- sample.int(n = nrow(data_excl), size = floor(perc_sample*nrow(data_excl)), replace = F)
data_train <- data_excl[sample, ]
data_test <- data_excl[-sample, ]
res<-list( "data_train" = data_train, "data_test"=data_test)
return(res)
}

## Call the function ##
## In case of a coded dataset (loaded for confirmatory purposes)
#data_all<-train_test_sets(data_coded, seed=123, perc_sample=0.10)
#data.train<-data_all$data_train

## In case of a dataset with labels
data_all<-train_test_sets(lgbti_sav, seed=123, perc_sample=0.70)
data.train<-data_all$data_train
data.test<-data_all$data_test

# data.test<-data_all$data_test
##lgbti_sav[is.na(lgbti_sav$open_at_work),509:510]
#summary(data.train$open_at_work)
#summary(data_all$data_test$open_at_work)
```

```{r predictors, echo=FALSE}
## 2nd Function: Subsetting the dataset for selecting target variable and predictors

data_target<-function(data, target_var, predictors){
data_sel <- data %>%
            filter(!is.na(!!as.name(target_var))) %>% ## do not consider rows where the predictor is NA
            select(c(!!as.name(target_var), !!(predictors)))

return(data_sel)
}
                             
## Call the function (currently a limited subset of predictors are considered for testing purposes)

## Training dataset
data.train.f<- data_target(data.train, target_var = "open_at_work", predictors = c("H1", "H2", "H3", "H6", "H7", "H8", "H9", "H10") )
## Test dataset
data.test.f<- data_target(data.test, target_var = "open_at_work", predictors = c("H1", "H2", "H3", "H6", "H7", "H8", "H9", "H10") )

## Check the output that includes all target variables
head(data.train.f)
## Check whether NAs are included in the response variable (normally now should have been removed)
table(data.train.f$open_at_work,useNA = "always")
table(data.test.f$open_at_work,useNA = "always")

## Other useful variables currenntly not included for testing purposes
## "H7:H10", "H13", "H16:H20"
#(EXCLUDE H6_1)
```

```{r acc_val, echo=FALSE}
## 3d Function: Check for acceptable values
##########################################
## NOTE: Throughout the dataset,we have the following special values:
## -999: Don't know
## -777: Does not apply to me
## -888: Prefer not to say
## -1: Missing (for limited questions, applied only to "Other" category: "Please specify". So it is not a problem)
## -2: Not applicable
##########################################
## We should check for "-2" values in the set of auxiliary variables; Those should be probably converted to NA's
## Check if we have more variables, whether we should also insert an argument for specifying specific columns to be applied
repl_val<-function(data, val){
data<- data %>% 
   na_if(val) ##replace with NAs, the indicated values
return(data)
}
## Call the function
## In case that we use the dataset with coded values (now checked for testing purposes)
#data.train.frep<-repl_val(data.train.f,c(-2))

## In case that we use the dataset with the labels
## Training dataset
data.train.frep<-repl_val(data.train.f,c("Not applicable"))
table(data.train.frep$open_at_work,useNA = "always")

## Test dataset
data.test.frep<-repl_val(data.test.f,c("Not applicable"))
table(data.test.frep$open_at_work,useNA = "always")
```
```{r reclass, echo=FALSE}
## 4th Function: Group response variable into two categories
regroup<-function(data, target_var){
data<- data %>% 
  mutate(!!as.name(target_var) := recode(!!as.name(target_var), 'Very open'="Open", 'Selectively open'="Open"))
return(data)
}

## Call the function
## In case that we use the dataset with the labels
## Training dataset
data.train.final<-regroup(data.train.frep, target_var="open_at_work")
table(data.train.final$open_at_work,useNA = "always")

## Test dataset
data.test.final<-regroup(data.test.frep, target_var="open_at_work")
table(data.test.final$open_at_work,useNA = "always")
```


```{r tree_ML_test, echo=FALSE}
## Decision tree algorithm -- CART
library(rpart)

#### NOTE: Explanation of the parameters in the rpart formula ###
# Cp: complexity parameter --> avoids splitting those nodes that are obviously not worthwhile. 
# Cp value shall be determined after “growing the tree” and the optimal value is used to “prune the tree.”
# Cp: The default value is 0.01
# Cp: imposes a penalty to the tree for having two many splits. The higher the cp, the smaller the tree.

# minsplit: The minimum split criterion is the minimum number of records that must be present in a node before a split can be attempted
#######################################################


## Step 1. Run the algorithm
#######################################################
## STOP: If we do not restrict the Cp parameter, then we will receive the error below:
## Error in plot.rpart(tree1) : fit is not a tree, just a root.
## This occurs when the independent variables do not provide enough information to grow the tree. See, for example, the help for rpart. control: "Any split that does not decrease the overall lack of fit by a factor of cp is not attempted." 
## Trying loosening the control parameters, may result in the tree growing beyond a root.
tree1<-rpart(formula = open_at_work ~ ., data=data.train.frep,    method = "class")
#######################################################

## Step 2. Run the algorithm (by relaxing the Cp criterion) --> this worked
## Note: We have worked with the data where "Not applicable" have been replaced as NAs. The resulting procedure is different if we leave "not applicable" in the dataset. 
# Assumption: The algorithm understands NAs in factor variables
set.seed(1)
tree1<-rpart(formula = open_at_work ~ ., data=data.train.frep, method = "class", parms= list(split = "information"), control = rpart.control(minsplit=1,cp=0.01, maxdepth=10 , usesurrogate= 0, maxsurrogate= 0))

## Step 3. Explore the results
## a. Complexity table (shows number of splits, errors, etc.)
## Currently the error in the last split is relatively high: 67%
## Correctly classifies 45% of the cases
printcp(tree1)
## b. Detailed splitting process, Variables' importance, errors, etc.
summary(tree1)
## c. Plot results
par(xpd = NA) # otherwise on some devices the text is clipped
plot(tree1)
text(tree1,cex=0.3)
## Another more fancy way of plotting the tree
library(rattle)
res.plot<-fancyRpartPlot(tree1, main="Decision Tree tree train$open_at_work")
## Another way of plotting, this library gives more possibilities for parameterisation: http://www.milbo.org/rpart-plot/prp.pdf
library(rpart.plot)
rpart.plot(tree1)
## d. Shows specifically variable importance
tree1$variable.importance
## e. Plot relative error
plotcp(tree1)

########################################################
### Run the procedure for the grouped target variable: "Open", "Hide LGBTI"
set.seed(1)
tree1b<-rpart(formula = open_at_work ~ ., data=data.train.final,  method = "class", parms= list(split = "information"), control = rpart.control(minsplit=1,cp=0.01, maxdepth=10 , usesurrogate= 0, maxsurrogate= 0))

## Step 3. Explore the results
## a. Complexity table (shows number of splits, errors, etc.)
## Currently the error in the last split is relatively high: 34%
## Missclassifies the 23% of cases
## Grouping has led to the improvement of the model performance
## BUT we have minimised parameters affecting "Hide LGBTI" 
printcp(tree1b)
## b. Detailed splitting process, Variables' importance, errors, etc.
summary(tree1b)
## c. Plot results
par(xpd = NA) # otherwise on some devices the text is clipped
plot(tree1b)
text(tree1b,cex=0.3)
## Another more fancy way of plotting the tree
library(rattle)
res.plot<-fancyRpartPlot(tree1b, main="Decision Tree tree train$open_at_work")
## Another way of plotting, this library gives more possibilities for parameterisation: http://www.milbo.org/rpart-plot/prp.pdf
library(rpart.plot)
rpart.plot(tree1b)
## d. Shows specifically variable importance
tree1b$variable.importance
## e. Plot relative error
plotcp(tree1b)

```

```{r pred, echo=FALSE}
# Make predictions on the test data and check the accuracy of the model
predicted.classes <- tree1 %>% 
    predict(data.test.frep, type = "class")

head(predicted.classes)

# Compute model accuracy rate on test data
mean(predicted.classes == data.test.frep$open_at_work)

```

```{r prune, echo=FALSE}
## Pruning the tree
## CONCEPT: An optimal Cp value can be estimated by testing different cp values and using cross-validation approaches to determine the corresponding prediction accuracy of the model. The best cp is then defined as the one that maximize the cross-validation accuracy 
# Pruning can be performed in the caret package
library(caret)
set.seed(123)

# Step 1. Fit the model on the training set
# trControl: to set up 10-fold cross validation
# Number: stands for the number of resampling iterations
# tuneLength: stands for the number of possible Cp values to evaluate (default=3)
tree2 <- train(open_at_work ~., data = data.train.frep, method = "rpart",
  trControl = trainControl("cv", number = 30),tuneLength = 50, na.action = na.omit)

# Step 2. Plot model accuracy vs different values of Cp (complexity parameter)
plot(tree2)

# Step 3.Print the best tuning parameter Cp that maximizes the model accuracy
tree2$bestTune

# Step 4. Plot the final tree model
## This did not work well, we end up with a tree with a limited number of nodes
par(xpd = NA)
plot(tree2$finalModel)
text(tree2$finalModel)

# Step 5. Make predictions on the test data and check the accuracy of the model
predicted.classes2 <- tree2 %>% 
    predict(data.test.frep)
head(predicted.classes)

# Step 6. Compute model accuracy rate on test data
mean(predicted.classes2 == data.test.frep$open_at_work)
## We end up with worse accuracy. Should be further checked
```

```{r pred, echo=FALSE}


```



### HAVE NOT WORKED IN DETAIL IN THE FOLLOWING. THOSE WERE INITIAL TRIALS
```{r tree2_ML, echo=FALSE,include=FALSE}
library(party)
set.seed(123)

#myformula<-open_at_work ~ H1:H6 + H7:H10 + H13 + H16:H20
tree3<-ctree(open_at_work ~., data = data.train.frep)
  

##explore results
## Shows the splitting process, Variables' importance
print(tree3)
# plot the resulting tree
plot(tree3,type="simple")


##table(predict(tree1),train_h$open_at_work) --> does not work , needs some processing
```



```{r tree_ML, echo=FALSE, include=FALSE}
library(randomForest)

tree4<-randomForest(open_at_work ~. ,data = data.train.frep, na.action = na.roughfix)
print(tree4)
importance(tree4)
```