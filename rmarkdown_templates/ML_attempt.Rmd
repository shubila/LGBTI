---
title: "ML attempt"
author: "asa"
date: '2021-02-12'
output: html_document
#knit: (function(inputFile, encoding) {
#  rmarkdown::render(inputFile, encoding = encoding,output_file = "ML.html", output_dir = "../paper") })
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
# code for checking predictors
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(summarytools)) install.packages("summarytools")
if(!require(sjlabelled)) install.packages("sjlabelled")
if(!require(kableExtra)) install.packages("kableExtra")
sapply(c("tidyverse","summarytools","sjlabelled", "kableExtra"), library, character.only = TRUE)
#library(tidyverse)
#library(summarytools)
#library(sjlabelled)
#library(kableExtra)

```

```{r interest_url, echo=FALSE, include=FALSE}
## https://www.r-bloggers.com/2019/06/working-with-spss-labels-in-r/
## Reference url for methods: https://www.statmethods.net/advstats/cart.html
## Reference for the code: http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials/
```

```{r data_label, echo=FALSE, include=FALSE}

# Step 1. Load data saved (following the "generate_clean_data.R)
lgbti_dta <- readRDS("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/clean/lgbti_dta.rds")
lgbti_sav <- readRDS("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/clean/lgbti_sav.rds")

# Step 2. Extract question labels from the .dta dataset and put them to the sav dataset
# a. Derive variable labels from the .dta dataset
var_labs <- map_chr(lgbti_dta, ~ attributes(.)$label)
# b. Attach labels to the .sav dataset
lgbti_sav <- set_label(lgbti_sav,var_labs)
```

```{r data_code, echo=FALSE, include=FALSE}
library(haven)
## Alternatively import data in coded format (haven::read_sav)
data_coded <- read_sav("~/Dropbox/My Mac (Anais iMac NEW)/Desktop/Master Project/Data/LGBTI/data/raw/ZA7604_v1-0-0.sav")

## Step 1. Quick look at the data format
data_coded %>% 
  .[,1:20] %>%  # First 20 columns
  glimpse() ## <dbl+lbl></dbl+lbl> signifies that these variables have labels associated to the numeric values they hold

## Step 2. Extract HTML document fully describing the data, their categories and answer options
library(sjPlot) #sjPlot is required
var_desc<-data_coded %>% sjPlot::view_df() 
print(var_desc)
```

```{r train_test_datasets, echo=FALSE, include=FALSE}
## 1st Function: First approach to create a training and test dataset
######################  
## NOTE: There are alternative ways of producing the train dataset, e.g. giving probabilities. Currently we keep it simple
######################  

train_test_sets<-function(data, seed, perc_sample){
## Data: input data to be subsetted to create the training and test datasets
## seed: Specify initial value for the seed
## perc_sample: the percentage of the sample size to be taken for specifying the size of the train dataset
set.seed(seed)
data_excl<- data %>% filter(!is.na(open_at_work)) ## Does this work for the labeled data?
sample <- sample.int(n = nrow(data_excl), size = floor(perc_sample*nrow(data_excl)), replace = F)
data_train <- data_excl[sample, ]
data_test <- data_excl[-sample, ]
res<-list( "data_train" = data_train, "data_test"=data_test)
return(res)
}

## Call the function ##
## In case of a coded dataset (loaded for confirmatory purposes)
#data_all<-train_test_sets(data_coded, seed=123, perc_sample=0.10)
#data.train<-data_all$data_train

## In case of a dataset with labels
data_all<-train_test_sets(lgbti_sav, seed=123, perc_sample=0.10)
data.train<-data_all$data_train

# data.test<-data_all$data_test
##lgbti_sav[is.na(lgbti_sav$open_at_work),509:510]
#summary(data.train$open_at_work)
#summary(data_all$data_test$open_at_work)
```

```{r predictors, echo=FALSE}
## 2nd Function: Subsetting the dataset for selecting target variable and predictors

data_target<-function(data, target_var, predictors){
data_sel <- data %>%
            filter(!is.na(!!as.name(target_var))) %>% ## do not consider rows where the predictor is NA
            select(c(!!as.name(target_var), !!(predictors)))

return(data_sel)
}
                             
## Call the function (currently a limited subset of predictors are considered for testing purposes)
data.train.f<- data_target(data.train, target_var = "open_at_work", predictors = c("H1", "H2", "H3", "H6", "H7", "H8", "H9", "H10") )
## Check the output that includes all target variables
head(data.train.f)
## Check whether NAs are included in the response variable (normally now should have been removed)
table(data.train.f$open_at_work,useNA = "always")

## Other useful variables currenntly not included for testing purposes
## "H7:H10", "H13", "H16:H20"
#(EXCLUDE H6_1)
#train_h<-train_h[1:5000,]
```

```{r acc_val, echo=FALSE}
## Check for acceptable values
##########################################
## NOTE: Throughout the dataset,we have the following special values:
## -999: Don't know
## -777: Does not apply to me
## -888: Prefer not to say
## -1: Missing (for limited questions, applied only to "Other" category: "Please specify". So it is not a problem)
## -2: Not applicable
##########################################
## We should check for "-2" values in the set of auxiliary variables; Those should be probably converted to NA's
## Check if we have more variables, whether we should also insert an argument for specifying specific columns to be applied
repl_val<-function(data, val){
data<- data %>% 
   na_if(val) ##replace with NAs, the indicated values
return(data)
}
## Call the function
## In case that we use the dataset with coded values (now checked for testing purposes)
#data.train.frep<-repl_val(data.train.f,c(-2))

## In case that we use the dataset with the labels
data.train.frep<-repl_val(data.train.f,c("Not applicable"))
table(data.train.frep$open_at_work,useNA = "always")
```


```{r tree_ML_test, echo=FALSE}
## Decision tree algorithm
library(rpart)
set.seed(123)
#### NOTE: Explanation of the parameters in the rpart formula ###
# Cp: complexity parameter --> avoids splitting those nodes that are obviously not worthwhile. 
# Cp value shall be determined after “growing the tree” and the optimal value is used to “prune the tree.”
# minsplit: The minimum split criterion is the minimum number of records that must be present in a node before a split can be attempted

## Step 1. Run the algorithm
#######################################################
## STOP: If we do not restrict the Cp parameter, then we will receive the error below:
## Error in plot.rpart(tree1) : fit is not a tree, just a root.
## This occurs when the independent variables do not provide enough information to grow the tree. See, for example, the help for rpart. control: "Any split that does not decrease the overall lack of fit by a factor of cp is not attempted." 
## Trying loosening the control parameters, may result in the tree growing beyond a root.
tree1<-rpart(formula = open_at_work ~ ., data=data.train.frep,    method = "class")
#######################################################

## Step 2. Run the algorithm (by relaxing the Cp criterion) --> this worked
## Note: We have worked with the data where "Not applicable" have been replaced as NAs. The resulting procedure is different if we leave "not applicable" in the dataset. 
# Assumption: The algorithm understands NAs in factor variables

tree1<-rpart(formula = open_at_work ~ ., data=data.train.frep, method = "class", parms= list(split = "information"), 
            control = rpart.control(minsplit=2,minbucket=1, cp=0.01, maxdepth=10 , usesurrogate= 0, maxsurrogate= 0))

## Step 3. Explore the results
## a. A short summary of the constructed tree, Variables used, Errors, std, etc.
printcp(tree1)
## b. Detailed splitting process, Variables' importance, errors, etc.
summary(tree1)
## c. Plot results
par(xpd = NA) # otherwise on some devices the text is clipped
plot(tree1)
text(tree1,cex=0.3)
## Another more fancy way of plotting the tree
library(rattle)
res.plot<-fancyRpartPlot(tree1, main="Decision Tree tree train$open_at_work")
## d. Shows specifically variable importance
tree1$variable.importance
## e. Plot relative error
plotcp(tree1)

```
### HAVE NOT WORKED IN DETAIL IN THE FOLLOWING. THOSE WERE INITIAL TRIALS
```{r tree2_ML, echo=FALSE,include=FALSE}
library(party)
set.seed(123)

myformula<-open_at_work ~ H1:H6 + H7:H10 + H13 + H16:H20
tree2<-ctree(myformula, data = train_h)
  

##explore results
## Shows the splitting process, Variables' importance
print(tree2)
# plot the resulting tree
plot(tree2,type="simple")


##table(predict(tree1),train_h$open_at_work) --> does not work , needs some processing
```



```{r tree_ML, echo=FALSE, include=FALSE}
library(randomForest)

tree3<-randomForest(myformula,data=train_h, na.action = na.roughfix)
print(tree3)
importance(tree3)
```